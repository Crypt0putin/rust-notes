# üï∑Ô∏è Web Scraper - –ü–∞—Ä—Å–µ—Ä –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü

–ü—Ä–æ–µ–∫—Ç –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è HTTP-–∫–ª–∏–µ–Ω—Ç–æ–≤, –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML, –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫.

## üéØ –¶–µ–ª–∏ –ø—Ä–æ–µ–∫—Ç–∞

### –ò–∑—É—á–∞–µ–º—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏  
- [[HTTP-Requests]] - HTTP –∑–∞–ø—Ä–æ—Å—ã —Å reqwest
- [[HTML-Parsing]] - –ü–∞—Ä—Å–∏–Ω–≥ HTML —Å scraper
- [[Async-Programming]] - –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ
- [[07-Error-Handling/Custom-Errors]] - –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ç–∏–ø—ã –æ—à–∏–±–æ–∫
- [[File-IO]] - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ñ–∞–π–ª—ã

### –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ crates
- `reqwest` - HTTP –∫–ª–∏–µ–Ω—Ç —Å async/await –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
- `scraper` - –ø–∞—Ä—Å–∏–Ω–≥ HTML –∏ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—ã  
- `tokio` - –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Å—Ä–µ–¥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
- `serde` - —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
- `clap` - –ø–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏

## üìã –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å

### MVP (Minimum Viable Product)
- ‚úÖ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
- ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
- ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON/CSV —Ñ–∞–π–ª
- ‚úÖ –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
- üéØ –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ URL –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
- üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
- üö´ Respect robots.txt  
- ‚è∞ –ó–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
- üîÑ –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
- üì± –†–∞–∑–Ω—ã–µ User-Agent

## üèóÔ∏è –ü–ª–∞–Ω —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏

### –≠—Ç–∞–ø 1: –ë–∞–∑–æ–≤—ã–π —Å–∫—Ä–∞–ø–µ—Ä
- [[Task-1-HTTP-Client]] - –ù–∞—Å—Ç—Ä–æ–π–∫–∞ HTTP –∫–ª–∏–µ–Ω—Ç–∞
- [[Task-2-HTML-Parser]] - –ü–∞—Ä—Å–∏–Ω–≥ HTML 
- [[Task-3-Data-Extraction]] - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö

### –≠—Ç–∞–ø 2: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
- [[Task-4-Async-Multiple]] - –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
- [[Task-5-Error-Handling]] - –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
- [[Task-6-CLI-Interface]] - –ö–æ–º–∞–Ω–¥–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

### –≠—Ç–∞–ø 3: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
- [[Task-7-Rate-Limiting]] - –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏
- [[Task-8-Caching]] - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- [[Task-9-Configuration]] - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

## üíª –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–¥–∞

### Cargo.toml
```toml
[package]
name = "web_scraper"
version = "0.1.0"
edition = "2021"

[dependencies]
reqwest = { version = "0.11", features = ["json"] }
scraper = "0.17"
tokio = { version = "1", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
csv = "1.2"
clap = { version = "4.0", features = ["derive"] }
anyhow = "1.0"
url = "2.4"
```

### –ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
```rust
use reqwest::Client;
use scraper::{Html, Selector};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use anyhow::{Result, anyhow};

#[derive(Debug, Serialize, Deserialize)]
struct ScrapedData {
    url: String,
    title: Option<String>,
    data: HashMap<String, Vec<String>>,
    scraped_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug)]
struct WebScraper {
    client: Client,
    selectors: HashMap<String, Selector>,
}

impl WebScraper {
    fn new() -> Self {
        let client = Client::builder()
            .user_agent("Web Scraper 1.0")
            .timeout(std::time::Duration::from_secs(30))
            .build()
            .expect("Failed to create HTTP client");
            
        Self {
            client,
            selectors: HashMap::new(),
        }
    }
    
    fn add_selector(&mut self, name: &str, selector: &str) -> Result<()> {
        let parsed_selector = Selector::parse(selector)
            .map_err(|e| anyhow!("Invalid CSS selector '{}': {:?}", selector, e))?;
        self.selectors.insert(name.to_string(), parsed_selector);
        Ok(())
    }
    
    async fn scrape(&self, url: &str) -> Result<ScrapedData> {
        // –°–∫–∞—á–∏–≤–∞–µ–º HTML
        let response = self.client.get(url).send().await?;
        let html_content = response.text().await?;
        let document = Html::parse_document(&html_content);
        
        // –ò–∑–≤–ª–µ–∫–∞–µ–º title
        let title_selector = Selector::parse("title").unwrap();
        let title = document
            .select(&title_selector)
            .next()
            .map(|element| element.text().collect::<Vec<_>>().join(" "));
        
        // –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
        let mut data = HashMap::new();
        for (name, selector) in &self.selectors {
            let elements: Vec<String> = document
                .select(selector)
                .map(|element| element.text().collect::<Vec<_>>().join(" "))
                .collect();
            data.insert(name.clone(), elements);
        }
        
        Ok(ScrapedData {
            url: url.to_string(),
            title,
            data,
            scraped_at: chrono::Utc::now(),
        })
    }
}
```

## üß™ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä
```rust
#[tokio::main]
async fn main() -> Result<()> {
    let mut scraper = WebScraper::new();
    
    // –î–æ–±–∞–≤–ª—è–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    scraper.add_selector("headings", "h1, h2, h3")?;
    scraper.add_selector("links", "a[href]")?;
    scraper.add_selector("paragraphs", "p")?;
    
    // –°–∫—Ä–∞–ø–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
    let data = scraper.scrape("https://example.com").await?;
    
    // –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    println!("Title: {:?}", data.title);
    println!("Found {} headings", data.data.get("headings").unwrap_or(&vec![]).len());
    
    // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
    let json = serde_json::to_string_pretty(&data)?;
    std::fs::write("scraped_data.json", json)?;
    
    Ok(())
}
```

### –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ URL
```rust
async fn scrape_multiple_urls(urls: Vec<&str>) -> Result<Vec<ScrapedData>> {
    let mut scraper = WebScraper::new();
    scraper.add_selector("title", "h1")?;
    scraper.add_selector("description", "meta[name='description']")?;
    
    // –°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    let tasks: Vec<_> = urls
        .into_iter()
        .map(|url| {
            let scraper = &scraper;
            async move {
                scraper.scrape(url).await
            }
        })
        .collect();
    
    // –ñ–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
    let results = futures::future::join_all(tasks).await;
    
    // –°–æ–±–∏—Ä–∞–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    let mut scraped_data = Vec::new();
    for result in results {
        match result {
            Ok(data) => scraped_data.push(data),
            Err(e) => eprintln!("Scraping error: {}", e),
        }
    }
    
    Ok(scraped_data)
}
```

## üöÄ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏

### 1. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –∏–∑ —Ñ–∞–π–ª–∞
```rust
use serde::{Deserialize, Serialize};

#[derive(Debug, Deserialize, Serialize)]
struct ScrapingConfig {
    selectors: HashMap<String, String>,
    delay_ms: Option<u64>,
    max_retries: Option<u32>,
    user_agent: Option<String>,
}

impl ScrapingConfig {
    fn load_from_file(path: &str) -> Result<Self> {
        let content = std::fs::read_to_string(path)?;
        let config: ScrapingConfig = serde_yaml::from_str(&content)?;
        Ok(config)
    }
}

// config.yaml
/*
selectors:
  title: "h1"
  price: ".price"
  description: ".description"
delay_ms: 1000
max_retries: 3
user_agent: "Custom Scraper 1.0"
*/
```

### 2. Respect robots.txt
```rust
use reqwest::Url;

struct RobotsChecker {
    client: Client,
    cache: HashMap<String, bool>,
}

impl RobotsChecker {
    async fn can_fetch(&mut self, url: &str, user_agent: &str) -> Result<bool> {
        let parsed_url = Url::parse(url)?;
        let base_url = format!("{}://{}", parsed_url.scheme(), parsed_url.host_str().unwrap());
        
        if let Some(&allowed) = self.cache.get(&base_url) {
            return Ok(allowed);
        }
        
        let robots_url = format!("{}/robots.txt", base_url);
        match self.client.get(&robots_url).send().await {
            Ok(response) => {
                if response.status().is_success() {
                    let robots_content = response.text().await?;
                    let allowed = self.parse_robots_txt(&robots_content, user_agent);
                    self.cache.insert(base_url, allowed);
                    Ok(allowed)
                } else {
                    Ok(true) // –ï—Å–ª–∏ robots.txt –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Ä–∞–∑—Ä–µ—à–∞–µ–º
                }
            }
            Err(_) => Ok(true),
        }
    }
    
    fn parse_robots_txt(&self, content: &str, user_agent: &str) -> bool {
        // –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä robots.txt
        let lines: Vec<&str> = content.lines().collect();
        let mut current_user_agent = "";
        let mut disallowed_paths = Vec::new();
        
        for line in lines {
            let line = line.trim();
            if line.starts_with("User-agent:") {
                current_user_agent = line.split(":").nth(1).unwrap_or("").trim();
            } else if line.starts_with("Disallow:") && 
                     (current_user_agent == "*" || current_user_agent == user_agent) {
                let path = line.split(":").nth(1).unwrap_or("").trim();
                disallowed_paths.push(path);
            }
        }
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–ø—Ä–µ—â—ë–Ω –ª–∏ –ø—É—Ç—å
        !disallowed_paths.iter().any(|&path| path == "/" || path == "/*")
    }
}
```

### 3. Rate limiting –∏ retry –ª–æ–≥–∏–∫–∞
```rust
use std::time::{Duration, Instant};
use tokio::time::sleep;

struct RateLimitedScraper {
    scraper: WebScraper,
    last_request: Option<Instant>,
    delay: Duration,
    max_retries: u32,
}

impl RateLimitedScraper {
    fn new(delay_ms: u64, max_retries: u32) -> Self {
        Self {
            scraper: WebScraper::new(),
            last_request: None,
            delay: Duration::from_millis(delay_ms),
            max_retries,
        }
    }
    
    async fn scrape_with_retry(&mut self, url: &str) -> Result<ScrapedData> {
        // Rate limiting
        if let Some(last) = self.last_request {
            let elapsed = last.elapsed();
            if elapsed < self.delay {
                sleep(self.delay - elapsed).await;
            }
        }
        
        // Retry logic
        let mut attempts = 0;
        loop {
            attempts += 1;
            self.last_request = Some(Instant::now());
            
            match self.scraper.scrape(url).await {
                Ok(data) => return Ok(data),
                Err(e) => {
                    if attempts >= self.max_retries {
                        return Err(e);
                    }
                    eprintln!("Attempt {} failed, retrying in {:?}...", attempts, self.delay);
                    sleep(self.delay * attempts).await; // Exponential backoff
                }
            }
        }
    }
}
```

## üéØ CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

```rust
use clap::{Args, Parser, Subcommand};

#[derive(Parser)]
#[command(name = "web_scraper")]
#[command(about = "A web scraping tool built with Rust")]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// Scrape a single URL
    Single(SingleArgs),
    /// Scrape multiple URLs from a file
    Batch(BatchArgs),
    /// Test CSS selectors on a page
    Test(TestArgs),
}

#[derive(Args)]
struct SingleArgs {
    /// URL to scrape
    url: String,
    /// CSS selector to extract data
    #[arg(short, long)]
    selector: String,
    /// Output file path
    #[arg(short, long, default_value = "output.json")]
    output: String,
}

#[derive(Args)]
struct BatchArgs {
    /// File with URLs (one per line)
    #[arg(short, long)]
    urls_file: String,
    /// Configuration file
    #[arg(short, long)]
    config: String,
    /// Output directory
    #[arg(short, long, default_value = "output")]
    output_dir: String,
}

#[tokio::main]
async fn main() -> Result<()> {
    let cli = Cli::parse();
    
    match cli.command {
        Commands::Single(args) => {
            let mut scraper = WebScraper::new();
            scraper.add_selector("data", &args.selector)?;
            
            let result = scraper.scrape(&args.url).await?;
            let json = serde_json::to_string_pretty(&result)?;
            std::fs::write(&args.output, json)?;
            
            println!("Scraped data saved to {}", args.output);
        }
        Commands::Batch(args) => {
            // –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –±–∞—Ç—á-—Ä–µ–∂–∏–º–∞
            println!("Batch scraping not implemented yet");
        }
        Commands::Test(args) => {
            // –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
            println!("Test mode not implemented yet");
        }
    }
    
    Ok(())
}
```

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ú–æ–¥—É–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tokio_test;

    #[tokio::test]
    async fn test_scraper_creation() {
        let scraper = WebScraper::new();
        assert_eq!(scraper.selectors.len(), 0);
    }

    #[tokio::test]
    async fn test_add_selector() {
        let mut scraper = WebScraper::new();
        assert!(scraper.add_selector("test", "h1").is_ok());
        assert!(scraper.add_selector("invalid", ">>invalid<<").is_err());
    }

    #[test]
    fn test_scraped_data_serialization() {
        let data = ScrapedData {
            url: "https://example.com".to_string(),
            title: Some("Test Title".to_string()),
            data: HashMap::new(),
            scraped_at: chrono::Utc::now(),
        };
        
        let json = serde_json::to_string(&data).unwrap();
        let _deserialized: ScrapedData = serde_json::from_str(&json).unwrap();
    }
}
```

## üéØ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è

### –ù–∞—á–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å
1. **–°–∫—Ä–∞–ø–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π**: –ò–∑–≤–ª–µ–∫–∞–π—Ç–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —Å–∞–π—Ç–∞
2. **–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–Ω**: –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ —Ü–µ–Ω—ã —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–µ  
3. **–°–±–æ—Ä —Å—Å—ã–ª–æ–∫**: –ò–∑–≤–ª–µ–∫–∞–π—Ç–µ –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã

### –°—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å
1. **–ú–Ω–æ–≥–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥**: –ü–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –ø–æ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
2. **–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ä–º**: –ó–∞–ø–æ–ª–Ω—è–π—Ç–µ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–π—Ç–µ —Ñ–æ—Ä–º—ã
3. **–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π**: –°–∫–∞—á–∏–≤–∞–π—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã

### –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —É—Ä–æ–≤–µ–Ω—å
1. **JavaScript —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ headless –±—Ä–∞—É–∑–µ—Ä –¥–ª—è SPA
2. **–û–±—Ö–æ–¥ –∑–∞—â–∏—Ç—ã**: –†–∞–±–æ—Ç–∞ —Å CAPTCHA –∏ –∞–Ω—Ç–∏-–±–æ—Ç —Å–∏—Å—Ç–µ–º–∞–º–∏
3. **–†–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥**: –ö–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏

## ‚ö†Ô∏è –≠—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è

### –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏
- **–£–≤–∞–∂–∞–π—Ç–µ robots.txt**
- **–ù–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞–π—Ç–µ —Å–µ—Ä–≤–µ—Ä–∞** - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∑–∞–¥–µ—Ä–∂–∫–∏
- **–°–æ–±–ª—é–¥–∞–π—Ç–µ Terms of Service**
- **–ö—ç—à–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã** –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤

### –ü—Ä–∞–≤–æ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã
- –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –∑–∞–∫–æ–Ω–Ω–æ—Å—Ç—å —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –≤ –≤–∞—à–µ–π —é—Ä–∏—Å–¥–∏–∫—Ü–∏–∏
- –ù–µ —Å–∫—Ä–∞–ø—å—Ç–µ –∞–≤—Ç–æ—Ä—Å–∫–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –±–µ–∑ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
- –£—á–∏—Ç—ã–≤–∞–π—Ç–µ GDPR –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏

## üîó –°–≤—è–∑–∞–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
- [[Async-Programming]] - –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ
- [[HTTP-Client-Patterns]] - –ü–∞—Ç—Ç–µ—Ä–Ω—ã HTTP –∫–ª–∏–µ–Ω—Ç–æ–≤
- [[Code-Snippets/Error-Handling-Patterns]] - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

#project #web-scraping #async #http #intermediate #automation